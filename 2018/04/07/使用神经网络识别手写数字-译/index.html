<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222"> 








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="aG59t0Ryn-VWUySoEgvtBTURUjB1vy4e3Fl30Qvemd4" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="python,神经网络," />










<meta name="description" content="使用神经网络识别手写数字python版本:python2">
<meta name="keywords" content="python,神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="使用神经网络识别手写数字[译]">
<meta property="og:url" content="http://www.vhcffh.com/2018/04/07/使用神经网络识别手写数字-译/index.html">
<meta property="og:site_name" content="昔日风">
<meta property="og:description" content="使用神经网络识别手写数字python版本:python2">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_digits.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_mnist_100_digits.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz0.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz1.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz2.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz3.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz4.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz5.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz6.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz7.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz8.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz9.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_sigmoid_graph.PNG">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_step_graph.PNG">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz10.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz11.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_digits.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_digits_separate.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_mnist_first_digit.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_tikz12.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_mnist_top_left_feature.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_mnist_other_features.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_mnist_complete_zero.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/9_9_tikz13.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/4_17_digits.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/9_9_valley.png">
<meta property="og:image" content="http://www.vhcffh.com/imgs2018/9_13_valley_with_ball.png">
<meta property="og:updated_time" content="2018-09-16T06:40:56.053Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用神经网络识别手写数字[译]">
<meta name="twitter:description" content="使用神经网络识别手写数字python版本:python2">
<meta name="twitter:image" content="http://www.vhcffh.com/imgs2018/4_17_digits.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.vhcffh.com/2018/04/07/使用神经网络识别手写数字-译/"/>




<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-3597458182538053",
    enable_page_level_ads: true
  });
</script>


  <title>使用神经网络识别手写数字[译] | 昔日风</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-111205240-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?738c8b07f562345c0a81e28d2ccb2ed1";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">昔日风</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Frey's blog</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-摘录">
          <a href="/quotes/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br />
            
            摘录
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.vhcffh.com/2018/04/07/使用神经网络识别手写数字-译/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frey">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="昔日风">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">使用神经网络识别手写数字[译]</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-07T17:45:48+08:00">
                2018-04-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/04/07/使用神经网络识别手写数字-译/" class="leancloud_visitors" data-flag-title="使用神经网络识别手写数字[译]">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                <span title="字数统计">
                  20,090
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                <span title="阅读时长">
                  74
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="使用神经网络识别手写数字"><a href="#使用神经网络识别手写数字" class="headerlink" title="使用神经网络识别手写数字"></a>使用神经网络识别手写数字</h1><p>python版本:python2<br><a id="more"></a></p>
<h2 id="1-第一章"><a href="#1-第一章" class="headerlink" title="1.第一章"></a>1.第一章</h2><p>人类的视觉系统是世界的奇迹之一，考虑如下的一串手写数字：<br><img src="/imgs2018/4_17_digits.png"><br>大多数人毫不费力地将这些数字识别为504192。这种安逸是骗人的，在我们大脑的每个半球，人类都有一个主要的视觉皮层，也被称为$V_1$，含有1.4亿个神经元，它们之间有数以百亿计的联系。而且，人类视觉不仅涉及$V_1$，还涉及到整个系列的视觉皮质 - $V_2$，$V_3$，$V_4$和$V_5$ - 逐渐进行更复杂的图像处理。我们的大脑中有一台超级计算机，根据数亿年的演变进行调整，并且非常适合了解视觉世界。识别手写数字并不容易。相反，我们人类是惊人的，非常擅长理解我们的眼睛向我们展示的东西。但几乎所有这些工作都是在无意识中完成的。所以我们通常不会意识到我们的视觉系统解决问题有多复杂。<br>如果您尝试编写计算机程序来识别上述数字，则视觉模式识别的难度就会变得明显。对于我们自己来说非常简单的事突然变得非常困难。关于我们如何识别形状的简单直觉 ——“一个9在顶部有一个循环，而在右下方有一个垂直的冲程” ——在算法上表达的结果并不是那么简单。当你试图精确地制定这样的规则时，你很快会迷失在例外和警告以及特殊情况之中。它似乎毫无希望。</p>
<p>神经网络用另一种方式解决这个问题。这个想法是采取大量的手写数字，称为训练示例，<br><img src="/imgs2018/4_17_mnist_100_digits.png"><br>然后开发一个可以从这些训练示例中学习的系统。换句话说，神经网络使用这些例子来自动推断识别手写数字的规则。此外，通过增加训练示例的数量，网络可以了解更多关于手写的内容，从而提高其准确性。所以，虽然我只显示了上面的100个训练数字，但也许我们可以使用数千甚至数百万或数十亿个训练示例来构建更好的手写识别器。在本章中，我们将编写一个计算机程序，实现一个学习识别手写数字的神经网络。该程序只有74行，并且不使用特殊的神经网络库。但是这个简短的程序识别数字的准确率超过96％，且无需人工干预。此外，在后面的章节中，我们将开发出将精度提高到99％以上的想法。事实上，最好的商业神经网络现在非常好，银行用它来处理支票，邮局用它来识别地址。</p>
<p>我们专注于手写识别，因为它对于一般学习神经网络来说是一个很好的原型问题。作为原型，它迎来了一个甜蜜点：它很具有挑战性 - 识别手写数字并不是一件容易的事情 - 但它并不是一个非常复杂的解决方案或者巨大的计算能力。此外，这是开发深度学习等更先进技术的好方法。因此，在整本书中，我们会一再回顾手写识别的问题。在本书后面，我们将讨论如何将这些想法应用于计算机视觉中的其他问题，以及语音，自然语言处理和其他领域。</p>
<p>当然，如果本章的要点只是编写一个计算机程序来识别手写数字，那么本章将会更短！但在学习中，我们将提出许多关于神经网络的关键思想，包括两种重要类型的人工神经元（感知器和Sigmoid神经元）和神经网络的标准学习算法，称为随机梯度下降。在整个过程中，我重点解释为什么事情按照他们的方式完成，并且建立你的神经网络直觉。这需要比如果我刚刚介绍发生的事情的基本机制更长的讨论，但它是值得的，以便您获得更深的理解。在学习与收获中，直到本章结束时，我们将能够理解深度学习是什么，为什么它很重要。</p>
<h3 id="1-1感知器（Perceptrons）"><a href="#1-1感知器（Perceptrons）" class="headerlink" title="1.1感知器（Perceptrons）"></a>1.1感知器（Perceptrons）</h3><p>什么是神经网络？首先，我将解释一种称为感知器的人造神经元。感知器是在20世纪50年代和60年代由科学家Frank Rosenblatt开发的，受到了Warren McCulloch和Walter Pitts早期工作的启发。今天，使用其他人造神经元模型更为常见 - 在本书中以及在神经网络上的大量现代工作中，使用的主要神经元模型是一种称为Sigmoid神经元的模型。我们马上就会去了解Sigmoid神经元。但要理解为什么Sigmoid神经元的如此定义，值得花时间先了解感知器。那么感知器如何工作？感知器需要几个二进制输入$x_1,x_2,x\ldots,$并且产生一个单一的二进制输出。<br><img src="/imgs2018/4_17_tikz0.png"><br>在上图的例子中，感知器有三个输入$x_1,x_2,x_3$。一般来说，它也可以有更多或更少的输入。Rosenblatt提出了一个简单的规则来计算输出。他介绍了权重$w_1,w_2,\ldots,$表示各个输入对输出重要性的实数。神经元的输出，0或者1，由权重和$\sum_jw_jx_j$比阈值更大还是更小确定。就像权重一样，阈值是一个实数，它是神经元的一个参数。用更精确的代数术语来描述：</p>
<script type="math/tex; mode=display">
output=\begin{cases}
0 & \text{如果$ \sum_jw_jx_j\le阈值 $}\\
1 & \text{如果$ \sum_jw_jx_j\gt阈值 $}
\end{cases}\tag{1}</script><p>这就是感知器如何工作的全部！</p>
<p>这是基本的数学模型。您可以考虑感知器的一种方式是，它是一种通过权衡证据来做出决定的设备。让我举个例子。这不是一个非常现实的例子，但它很容易理解，我们很快会得到更现实的例子。假设周末即将到来，你听说你的城市将会有一场奶酪节。你喜欢奶酪，并试图决定是否参加这个节日。您可以通过权衡三个因素来做出决定：</p>
<p>1.天气好吗？<br>2.你的男朋友或女朋友是否想陪你？<br>3.节日临近公共交通吗？ （你没有一辆车）。</p>
<p>我们可以用相应的二元变量$x_1,x_2$和$x_3$来表示这三个因子。例如，如果天气好，我们取$x_1=1$，如果天气不好，$x_1=0$。同样的，如果你的男友或女友也想去，$x_2=1$；$x_3$和公共交通也一样。</p>
<p>现在，假设你绝对喜欢奶酪，那么即使你的男朋友或女朋友不感兴趣并且节日很难到达，你也很乐意去参加这个音乐节。但是，也许你真的厌恶恶劣天气，如果天气不好，你不可能去参加这个节日。您可以使用感知器来对这种决策进行建模。一种方法是为天气选择权重$w_1=6$，对于其他条件$w_2=2$和$w_3=2$。$w_1$的值较大表示天气对你来说很重要，远远大于你的男友或女友是否加入你或公共交通的接近程度。最后，假设你为感知器选择了一个5的阈值。通过这些选择，感知器实现所需的决策模型，每当天气良好时输出1，当天气不好时输出0。无论你的男朋友还是女朋友想去，或者公共交通是否在附近，输出结果都没有区别。</p>
<p>通过改变权重和阈值，我们可以得到不同的决策模型。例如，假设我们选择了3的阈值。然后感知器会决定你应该在天气好的时候去参加音乐节，或者节日在公共交通附近，你的男朋友或女朋友愿意加入你。换句话说，这将是一个不同的决策模式。降低门槛意味着你更愿意去参加这个节日。</p>
<p>显然，感知器不是一个完整的人类决策模型！但是，这个例子说明了感知器如何权衡不同种类的证据以作出决定。似乎有理由认为，复杂的感知器网络可能会做出相当微妙的决定：<br><img src="/imgs2018/4_17_tikz1.png"><br>在这个网络中，第一列感知器 - 我们称之为第一层感知器 - 通过权衡输入证据，做出三个非常简单的决定。第二层感知器呢？这些感知器中的每一个都通过对第一层决策结果进行权衡来做出决定。通过这种方式，第二层中的感知器可以在比第一层中的感知器更复杂和更抽象的层次上做出决定。第三层感知器可以做出更复杂的决定。通过这种方式，感知器的多层网络可以进行复杂的决策。</p>
<p>顺便说一下，当我定义感知器时，我说感知器只有一个输出。在感知器之上的网络看起来他们有多个输出。事实上，他们仍然是单一输出。多个输出箭头仅仅是一个有用的方式，表明感知器的输出被用作其他几个感知器的输入。它比绘制一条单一的输出线然后分裂更加简洁。<br><small>2018-04-07 19:12:03</small><br>让我们简化描述感知器的方式。条件$\sum_jw_jx_j\gt$阈值显得复杂，我们可以通过两个简单的记法简化它。第一个是把$\sum_jw_jx_j$写作一个点积，$w\cdot x\equiv\sum_jw_jx_j$，其中，$w$和$x$都是向量，他们分别包含权重和输入。第二个变化是移动阈值到等式的左边，并用感知器的偏置代替它，$b=-threshold$(阈值)。使用偏置代替阈值，感知器的规则可以被重新定义为：</p>
<script type="math/tex; mode=display">
output=\begin{cases}
0 & \text{如果$ w\cdot x\le阈值 $}\\
1 & \text{如果$ w\cdot x\gt阈值 $}
\end{cases}\tag{2}\label{(2)}</script><p>你可以将偏置看作衡量感知器输出1的容易程度。或者用更多的生物学术语来说，偏置是衡量让感知器触发的容易程度。对于具有非常大偏置的感知器，感知器输出1是非常容易的。但是如果偏置非常小，那么感知器很难输出1。显然，引入偏置只是我们描述感知器的一个小变化，但是我们稍后会看到它会导致进一步的符号简化。因此，在本书的其余部分我们不会使用阈值，我们将始终使用偏置。</p>
<p>我将感知器描述为衡量证据作出决定的一种方法。另一种可以使用感知器的方法是计算我们通常认为是最基础的基本逻辑函数（与，或，与非）。例如，假设我们有一个有两个输入的感知器，每个输入的权重是-2，和一个整体的偏置3，下图是我们的感知器：<br><img src="/imgs2018/4_17_tikz2.png"><br>然后我们将看到输入[0,0]时，输出1，因为$(-2)\ast 0+(-2)\ast 0+3=3$是正数。在这里，引入$\ast$作为乘号。类似的计算能显示输入[0,1]和[1,0]会输出1.但是输入[1,1]将输出0。因此我们的感知器实现了与非门。</p>
<p>与非门示例显示我们可以使用感知器来计算简单的逻辑函数。实际上，我们可以使用感知器网络来计算任何逻辑函数。原因在于与非门是计算通用的，也就是说，我们可以在与非门外建立任何计算。例如，我们可以使用与非门来构建一个电路，该电路添加两个位$x_1$和$x_2$。若需要计算按位和，$x_1\oplus x_2$，以及当$x_1$和$x_2$都是1时进位位被设置为1，即进位位是位乘积$x_1x_2$：<br><img src="/imgs2018/4_17_tikz3.png"><br>为了获得感知器的等效网络，我们用感知器替换所有与非门，其中两个输入的权重均为-2，整体偏差为3。<br><img src="/imgs2018/4_17_tikz4.png"><br>这个感知器网络的一个值得注意的方面是，最左感知器的输出被用作最下感知器的双重输入。当我定义感知器模型时，我没有说是否允许这种双重输出到同一个地方。其实，这并不重要。如果我们不想允许这样的事情，那么可以简单地将这两条线合并成一个权重为-4的单一连接，而不是两个具有-2权重的连接。（如果你没有发现这一点，你应该停下来向自己证明这是相同的。）随着这种变化，网络看起来如下所示，所有未标记的权重等于-2，所有偏差等于3，单个权重为-4，如下所示：<br><img src="/imgs2018/4_17_tikz5.png"><br>到目前为止，我一直在绘制像$x_1$和$x_2$这样的输入作为感知器网络左侧的变量。事实上，传统的做法是绘制额外的感知层-输入层-对输入进行编码：<br><img src="/imgs2018/4_17_tikz6.png"><br>输入感知器的这种表示法，其中有输出，但没有输入，<br><img src="/imgs2018/4_17_tikz7.png"><br>上图是一个简写。它实际上并不意味着是没有输入的感知器。要看到这个，假设我们确实有一个没有输入的感知器。那么加权总和$\sum_jw_jx_j$总是为零，所以对于感知器，当$b\gt0$将输出1，当$b\le0$则输出0。也就是说，感知器只会输出一个固定的值，而不是所需的值（在上面的例子中是$x_1$）。最好将输入感知器视为不是真正的感知器，而是将特定的单位简单地定义为输出所需的值$x_1,x_2,x_3\ldots$</p>
<p>加法器实例演示了如何使用感知器网络来模拟包含许多与非门的电路。由于与非门在计算上是通用的，因此感知器对于计算也是普遍的。</p>
<p>感知器的计算普遍性同时令人放心和失望。这令人放心，因为它告诉我们，感知器网络可以像其他任何计算设备一样强大。但它也令人失望，因为它使得它看起来好像感知器仅仅是一种新型的与非门。这不是什么大新闻！</p>
<p>但是，情况比这个观点表明的要好。事实证明，我们可以设计出可以自动调整人造神经元网络的权重和偏差的学习算法。这种调整是针对外部刺激而发生的，没有程序员的直接干预。这些学习算法使我们能够以与传统逻辑门完全不同的方式使用人造神经元。我们的神经网络不是明确地布局与非电路和其他门电路，而是简单地学会解决问题，因为有时候直接设计传统电路非常困难。<br><small>2018-04-08 22:29:52</small></p>
<h3 id="1-2Sigmoid神经元（Sigmoid-neurons）"><a href="#1-2Sigmoid神经元（Sigmoid-neurons）" class="headerlink" title="1.2Sigmoid神经元（Sigmoid neurons）"></a>1.2Sigmoid神经元（Sigmoid neurons）</h3><p>学习算法听起来很棒。但是我们怎样才能为神经网络设计这样的算法呢？假设我们有一个想用来学习解决某个问题的感知器网络。例如，网络的输入可能是来自手写的数字扫描图像的原始像素数据。我们希望网络学习权重和偏差，以便网络正确地输出对数字的分类。要了解学习如何起作用，假设我们对网络中的一些权重（或偏差）进行小改动。我们想要的是，这种小的重量变化只会导致网络输出的相应变化很小。正如我们稍后会看到的，这个属性将使学习成为可能。简单来说，这就是我们想要的（显然这个网络太简单了，无法进行手写识别！）：<br><img src="/imgs2018/4_17_tikz8.png"><br>如果权重（或偏差）的小变化确实只会导致输出的小变化，那么我们可以利用这个事实来修改权重和偏差，从而使我们的网络以我们想要的方式行为运行。例如，假设当网络应该是“9”时，网络错误地将图像分类为“8”。我们可以弄清楚如何对权重和偏差做一些小的改变，这样网络就会更接近将图像分类为“9”。然后我们再重复一遍，改变权重和偏差以产生更好的输出。网络将在学习。</p>
<p>问题是，当我们的网络包含感知器时，这不是要发生的。事实上，网络中任何单个感知器的权重或偏差的小改变有时会导致该感知器的输出完全翻转，例如从0到1。这个翻转可能会导致网络其他部分的行为以一种非常复杂的方式完全改变。因此，虽然现在您的“9”可能被正确分类，但网络在所有其他图像上的行为可能会以一些难以控制的方式完全改变。这使得很难看到如何逐渐修改权重和偏差，以使网络更接近所需的行为。也许有一些解决这个问题的巧妙方法。但是，我们如何才能获得感知器的学习网络并不明显。我们可以通过引入一种称为sigmoid神经元的新型人造神经元来克服这个问题。sigmoid神经元与感知器类似，但是经过修改使得它们的权重和偏差的小变化仅导致其输出的小变化。这是允许一个sigmoid神经元网络学习的关键事实。</p>
<p>好的，让我描述sigmoid神经元。我们将用描述感知器的相同方式描述sigmoid神经元：<br><img src="/imgs2018/4_17_tikz9.png"><br>就像一个感知器，这个sigmoid神经元的输入为$X1,X2,\ldots$ </p>
<p>但这些输入不仅仅是0或1，而是可以取0和1之间的任何值。因此，例如，$0.638\ldots$是sigmoid神经元的有效输入。也就像感知器一样，sigmoid神经元对每个输入都有权重$w1,w2,\ldots$和整体偏差$b$。但输出不是0或1。取而代之的是$\sigma(w\cdot x+b)$,其中$\sigma$被称作sigmoid函数，它被定义为：</p>
<script type="math/tex; mode=display">
\sigma(z)\equiv{1 \over {1+e^{-z}}}\tag{3}\label{(3)}</script><p>为了更清楚地说明，具有输入$x_1,x_2,\ldots,$权重$w_1,w_2,\ldots,$的sigmoid神经元的偏置$b$为：</p>
<script type="math/tex; mode=display">
{1 \over 1+exp({-\sum_jw_jx_j-b})}\tag{4}\label{(4)}</script><p>乍一看，sigmoid神经元与感知器显得非常不同。如果你不熟悉sigmoid函数的代数形式，会感觉晦涩难以理解。事实上，在sigmoid神经元和感知器之间有许多相似之处，而sigmoid函数的代数形式更像一个技术细节，而不是真正的理解障碍。</p>
<p>为了理解它与感知器模型的相似之处，假设$z\equiv w\cdot x + b$是一个很大的正数，那么$e^{-z}\approx 0$，因此$sigma(z)\approx 1$。总之，当$z\equiv w\cdot x + b$是大的正数时，sigmoid神经元的输出接近1。就像一个感知器一样。相反，假设$z\equiv w\cdot x + b$是非常小的负数，那么$e^{-z}\to \infty$，而且$sigma(z)\approx 0$。因此当$z\equiv w\cdot x + b$是很小的负数时，sigmoid神经元的行为也接近一个感知器。只有当$ w\cdot x + b$的大小适中它的行为才会与感知器模型有很大区别。</p>
<p>关于$\sigma$函数的代数形式，我们应该怎样理解？事实上，$\sigma$的确切形式并不重要—重要的是这个函数的图形。下图是它的图形：<br><img src="/imgs2018/4_17_sigmoid_graph.PNG"><br>这个形状是一个平滑的阶梯函数版本：<br><img src="/imgs2018/4_17_step_graph.PNG"><br>事实上，如果$\sigma$函数是一个阶梯函数那么sigmoid神经元将是一个感知器，因为此时的输出0或者1取决于$w\cdot x+b$是正是负。如上所述，通过使用实际的$\sigma$函数，我们可以得到一个平滑的感知器。的确，$\sigma$函数的平滑性是至关重要的事实，而不是其详细的形式。$\sigma$函数的平滑性意味着权重的一个小变化$\Delta w_j$和偏置的小变化$\Delta b_j$在神经元的输出上也产生一个小变化$\Delta output$。事实上，通过微积分我们可以知道$\Delta output$很接近：</p>
<script type="math/tex; mode=display">
\begin{eqnarray} 
  \Delta output \approx \sum_j \frac{\partial output}{\partial w_j}
  \Delta w_j + \frac{\partial output}{\partial b} \Delta b
\tag{5}\label{(5)}
\end{eqnarray}</script><p>总和超过所有权重，$w_j$，${\partial output\over \partial w_j}$和${\partial output\over \partial b}$分别表示$output$对$w_j$和$b$的偏导数。如果你不喜欢偏微分等一系列复杂的东西，也无需惊慌。虽然上面的表达看起来很复杂，但是所有的偏导数，其实都是在说一些非常简单的事情（这是一个非常好的消息）：$\Delta output$是关于权重的变化$\Delta w_j$和偏置的变化$\Delta b$的一个线性函数。这种线性使得很容易选择权重和偏差的小变化来实现任何想得到的输出的小变化。因此，尽管sigmoid神经元和感知器有很多相同的定性行为，但他们使它更容易弄清楚偏置和权重的变化如何改变输出的变化。</p>
<p>如果只是$\sigma$的形状重要，而不是它的特定形式，那么在等式$\color{lime}{\eqref{(3)}}$中我们为什么使用$\sigma$函数的特定形式呢？事实上，在本书后面我们偶尔会考虑一些其它的激活函数$f(\cdot )$的输出为$f(w\cdot x+b)$的神经元。当我们使用不同的激活函数时，主要的变化是方程$\color{lime}{\eqref{(5)}}$中偏导数的特定值改变。事实证明，当我们稍后计算这些偏导数时，使用$\sigma$将简化计算，仅仅因为指数在计算时具有特别的性质。无论如何，$\sigma$通常用于神经网络的工作，并且是本书中最常用的激活函数。</p>
<p>我们应该如何解释sigmoid神经元的输出？显然，感知器和sigmoid神经元之间的一个很大的区别是sigmoid神经元不仅输出0或1。它们可以输出0到1之间的任何实数，因此$0.173\ldots$和$0.689\ldots$等值是合法输出。 例如，如果我们想要使用输出值来表示输入到神经网络的图像中像素的平均强度，这可能很有用。但有时它可能有干扰。假设我们希望来自网络的输出指示“输入图像是9”或“输入图像不是9”。显然，如果输出是0或1，那么最容易做到这一点，就像感知器一样。但实际上，我们可以设定一个约定来处理这个问题，例如，通过决定将至少为$0.5$的任何输出解释为指示“9”，并且将小于$0.5$的任何输出解释为指示“不是9 ”。我会一直明确地说出我们何时使用这样的惯例，所以它不应该引起任何混淆。</p>
<h3 id="1-3练习"><a href="#1-3练习" class="headerlink" title="1.3练习"></a>1.3练习</h3><ul>
<li><strong>第一部分，sigmoid神经元模拟感知器</strong><br>假设我们将所有的权重和偏差放在感知器网络中，并将它们乘以一个正常数$c\gt 0$。证明网络的行为不会改变。</li>
<li><strong>第二部分，sigmoid神经元模拟感知器</strong><br>假设我们具有与最后一个问题相同的设置的感知器网络。还假设已经选择了感知器网络的整体输入。我们不需要实际的输入值，我们只需要对输入进行修改。假设权重和偏差是这样的：对于网络中任何特定感知器的输入$x$，$w\cdot x+b\neq 0$。现在用sigmoid神经元替换网络中的所有感知器，并将权重和偏差乘以正常数$c\gt 0$。证明在极限$c \rightarrow \infty$时，这个S形神经元网络的行为与感知器网络完全相同。当一个感知器的$w\cdot x+b=0$时，这怎么会失败？</li>
</ul>
<p><small>2018-04-11 15:47:15</small></p>
<h3 id="1-4神经网络的体系结构"><a href="#1-4神经网络的体系结构" class="headerlink" title="1.4神经网络的体系结构"></a>1.4神经网络的体系结构</h3><p>在下一节中，我将介绍一个可以对手写数字进行分类的神经网络。为此做好准备，让我们为网络的不同部分命名，这有助于解释一些术语。假设我们有网络：<br><img src="/imgs2018/4_17_tikz10.png"><br>如前面提到的，该网络中最左边的层称为输入层，层内的神经元称为输入神经元。最右边或输出层包含输出神经元，或者，在这种情况下为单个输出神经元。中间层被称为隐藏层，因为该层中的神经元既不是输入也不是输出。“隐藏”这个词或许听起来有点神秘 - 第一次听到这个词我认为它必须有一些深刻的哲学或数学意义 - 但它只不过意味着“不是输入或输出”。上面的网络只有一个隐藏层，但有些网络有多个隐藏层。例如，以下四层网络有两个隐藏层：<br><img src="/imgs2018/4_17_tikz11.png"><br>有些令人困惑的是，由于历史原因，这种多层网络有时被称为多层感知器或MLPs，尽管是由sigmoid神经元组成的，而不是感知器。我不打算在本书中使用MLP术语，因为我觉得它很混乱，但是想警告你它的存在。</p>
<p>网络中输入和输出层的设计通常很简单。例如，假设我们试图确定手写图像是否描绘了“9”。设计网络的一种自然方式是将图像像素的强度编码成输入神经元。如果图像是$64 \times 64$的灰度图像，则我们有$4096 = 64 \times 64$的输入神经元，强度在0和1之间适当缩放。输出层将只包含一个神经元，输出值小于$0.5$表示“输入图像不是9”，大于$0.5$的值表示“输入图像是9”。虽然神经网络的输入和输出层的设计通常是直截了当的，但隐藏层的设计可能有相当的艺术。特别是，用几个简单的经验法则来概括隐藏层的设计过程是不可能的。相反，神经网络研究人员已经为隐藏层开发了许多设计启发式方法，这些方法可以帮助人们从他们的网络中获得他们想要的行为。例如，可以使用这种启发式方法来帮助确定如何根据训练网络所需的时间权衡隐藏层的数量。本书稍后会介绍几种这样的设计启发式。</p>
<p>到目前为止，我们一直在讨论神经网络，其中一层的输出被用作下一层的输入。这种网络被称为前馈神经网络。这意味着网络中没有环路 - 信息总是前馈，永不反馈。如果我们确实有循环，最终会出现$\sigma$函数的输入取决于输出的情况。这很难理解，所以我们不允许这样的循环。</p>
<p>但是，还有其他可以反馈回路的人工神经网络模型。这些模型被称为递归神经网络。这些模型中的想法是让神经元在静止之前在有限的时间内发射。这种射击可以刺激其他神经元，这些神经元可能会在稍后发射，持续时间也有限。这会导致更多的神经元发射，所以随着时间的推移，我们会得到一连串的神经元发射。循环不会在这样的模型中产生问题，因为神经元的输出仅在稍后时间而不是瞬时影响其输入。</p>
<p>递归神经网络比前馈网络的影响力小，部分原因是递归网络的学习算法（至少到目前为止）不那么强大。但递归网络仍然非常有趣。他们在精神上更接近我们的大脑如何工作，而不是前馈网络。有可能的是，递归网络可以解决重要问题，而这些问题只能通过前馈网络很难解决。</p>
<h3 id="1-5一个简单的网络来分类手写数字"><a href="#1-5一个简单的网络来分类手写数字" class="headerlink" title="1.5一个简单的网络来分类手写数字"></a>1.5一个简单的网络来分类手写数字</h3><p>定义了神经网络之后，让我们回到手写数字识别的实现。我们可以将识别手写数字的问题分成两个子问题。首先，我们想要一种将包含许多数字的图像分解为一系列单独图像的方式，每个图像都包含一个数字。例如，我们想要分解这个图片：<br><img src="/imgs2018/4_17_digits.png"><br>分成六个独立的图像，<br><img src="/imgs2018/4_17_digits_separate.png"><br>我们人类轻松地解决了这个分割问题，但是计算机程序正确分割图像是具有挑战性的。举例来说，我们希望我们的程序能够识别出上面的第一个数字，<br><img src="/imgs2018/4_17_mnist_first_digit.png"><br>是一个5。</p>
<p>我们将着重编写一个程序来解决第二个问题，即对单个数字进行识别。我们这样做是因为事实证明，一旦你有一种很好的分类单个数字的方法，分割问题就不难解决了。有很多方法可以解决分割问题。一种方法是尝试许多不同的分割图像的方式，使用一个数字分类器对每个分割方式进行评分。如果那个数字分类器对所有数字的分类准确率都很高，则分割方式得到高分;如果分类器在一个或多个数字分类中遇到很多问题，则分数较低。这个想法是，如果分类器在某处遇到麻烦，那么可能会遇到麻烦的原因是分割选择不正确。</p>
<p>要识别单个数字，我们将使用三层神经网络：<br><img src="/imgs2018/4_17_tikz12.png"><br>网络的输入层包含编码输入像素值的神经元。正如下一节所讨论的，我们的网络训练数据将包含许多扫描$28×28$像素的手写数字图像，因此输入层包含$784=28×28$个神经元。为了简单起见，我省略了上图中784个输入神经元中的大部分神经元。输入像素是灰度值，值为0.0表示白色，值为1.0表示黑色，在0和1之间的值表示逐渐变暗的灰色阴影。</p>
<p>网络的第二层是一个隐藏层。我们用$n$表示隐藏层中神经元的数量，我们将尝试$n$的不同值。显示的例子是了一个小的隐藏层，只包含$n=15$个隐藏层神经元。</p>
<p>网络的输出层包含10个神经元。如果第一个神经元触发，即输出$\approx 1$，那么这将表明网络认为该数字是0。如果第二个神经元触发，则表明网络认为该数字是1。一次类推。更精确一点，我们从0到9对输出神经元进行编号，并找出哪个神经元具有最高的激活值。如果这个神经元是第6个神经元，那么我们的网络会猜测输入数字是6。其他输出神经元亦是如此。</p>
<p>你可能想知道为什么我们使用1010个输出神经元。毕竟，网络的目标是告诉我们输入图像对应于哪个数字$(0,1,2,\ldots 9)$。看似自然的做法是只使用4个输出神经元，根据神经元的输出是接近于0还是接近1来将每个神经元视为二进制值。四个神经元足以对答案进行编码，因为$2^4=16$比输入数字的个数10多。为什么我们的网络应该使用10个神经元呢？这不是没有效率吗？最终的理由来自于经验：我们可以尝试两种网络设计，结果发现，对于这个特定的问题，具有10个输出神经元的网络学习识别数字比具有4个输出神经元的网络更好。但是这让我们想知道为什么使用10输出神经元的效果更好。是否有一些启发式方法会提前告诉我们应该使用10个输出编码而不是4输出编码？</p>
<p>通过理解我们为什么这样做，它有助于根据第一原则思考神经网络正在做什么。首先考虑使用10个输出神经元的情况。让我们专注于第一个输出神经元，一个试图决定数字是否为0的神经元。它通过权衡来自神经元隐藏层的证据来做到这一点。那些隐藏的神经元在干什么？很好，为了说明这一点，假设隐藏层中的第一个神经元检测是否存在如下图像：<br><img src="/imgs2018/4_17_mnist_top_left_feature.png"><br>它可以通过对与图像重叠的输入像素进行大量加权来实现，并且只对其他输入轻微加权。以类似的方式，为了论证的缘故，我们假设隐藏层中的第二，第三和第四个神经元检测是否存在以下图像：<br><img src="/imgs2018/4_17_mnist_other_features.png"><br>正如您可能已经猜到的那样，这四幅图像一起组成了我们在前面显示的数字中看到的0图像：<br><img src="/imgs2018/4_17_mnist_complete_zero.png"><br>所以如果这四个隐藏的神经元都在触发，那么我们可以得出这个数字是0的结论。当然，这不是我们可以用来得出图像是0的唯一证据，我们可以通过许多其他方式（通过翻转上述图像或轻微扭曲）合理地获得0。但似乎可以肯定地说，至少在这种情况下，我们认为输入是0。<br><small>2018-04-12 22:17:33</small><br>假设神经网络按这种方式工作，我们合理的解释为什么$10$个输出神经元更好，而不是$4$个。如果我们有$4$个输出神经元，那么第一个输出神经元将试着确定什么是数字图像中最重要的位。这里没有很容易的方式来得到如上图所示简单的形状的重要位。很难想象，这里存在任何一个很好的根据来说明数字的形状组件与输出重要位的相关性。</p>
<p>现在，就像所说的那样，这就是一个启发式。没有什么能说明三层神经网络会按照我描述的那样运作，这些隐含层能够确定简单的形状组件。可能一个更聪明的学习算法将找到权重赋值以便我们使用$4$个输出神经元。但是我描述的这个启发式方法已经运作很好，能够节约很多时间来设计更好的神经元网络架构。</p>
<h3 id="1-6练习"><a href="#1-6练习" class="headerlink" title="1.6练习"></a>1.6练习</h3><p>有一个方法能够对上面的三层网络增加一个额外层来确定数值位表示。这个额外层将先前输出层转换为二进制表示，就像下图说明一样。找到这层新的输出神经元权重和偏移。假定第$3$层神经元（例如，原来的输出层）正确的输出有至少$0.99$的激励值，不正确的输出有小于$0.01$的激励值。<br><img src="/imgs2018/9_9_tikz13.png"></p>
<h3 id="1-7梯度下降学习算法"><a href="#1-7梯度下降学习算法" class="headerlink" title="1.7梯度下降学习算法"></a>1.7梯度下降学习算法</h3><p>现在我们已经有一个设计好的神经网络，它怎样能够学习识别数字呢?首要的事情是我们需要一个用于学习的数据集——也叫做训练数据集。我们将使用MNIST数据集，它包含成千上万的手写数字图像，同时还有它们对应的数字分类。MNIST的名字来源于NIST（美国国家标准与技术研究所）收集的两个修改后的数据集。这里是一些来自于MNIST的图像：<br><img src="/imgs2018/4_17_digits.png"><br>就像你看到的，事实上这些数字和本章开始展示的用于识别的图像一样。当然，测试我们的网络时候，我们会让它识别不在训练集中的图像！</p>
<p>MNIST数据来自两部分。第一部分包含60000幅用于训练的图像。这些图像是扫描250位人员的手写样本得到的，他们一半是美国人口普查局雇员，一半是高中学生。这些图像都是28乘28像素的灰度图。MNIST数据的第二部分包含10000幅用于测试的图像。它们也是28乘28的灰度图像。我们将使用这些测试数据来评估我们用于学习识别数字的神经网络。为了得到很好的测试性能，测试数据来自和训练数据不同的250位人员（仍然是来自人口普查局雇员和高中生）。 这帮助我们相信这个神经网络能够识别在训练期间没有看到的其他人写的数字。</p>
<p>我们将使用符号$x$来表示训练输入。可以方便的把训练输入$x$当作一个$28 \times 28 = 784$维的向量。每一个向量单元代表图像中一个像素的灰度值。我们将表示对应的输出为$y = y(x)$，其中$y$是一个$10$维向量。比如，如果一个特殊的训练图像$x$，表明是$6$，那么$y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0,0)^T$就是网络期望的输出。注意$T$是转置运算，将一个行向量转换为列向量。</p>
<p>我们想要的是一个能让我们找到合适的权重和偏移的算法，以便网络输出$y(x)$能够几乎满足所有训练输入$x$。为了量化这个匹配度目标，我们定义了一个代价函数:</p>
<script type="math/tex; mode=display">
C(w,b)\equiv \frac {1}{2n}\sum_x{||y(x)-a||}^2\tag{6}\label{(6)}</script><p>这里，$w$表示网络中的所有权重，$b$是所有偏移，$n$训练输入的总数，$a$是网络输入为$x$时的输出向量，总和是对所有输入$x$进行的累加。当然输出$a$取决于$x$，$w$和$b$，但是为了符号简化，我没有指明这种依赖关系。符号$| v |$只是表示向量$v$的长度。我们称$C$为二次型代价函数；它有时候也叫做均方误差或MSE。检验二次型代价函数，我们能看到$C(w,b)$是一个非负值，因为求和中的每一项都是非负的。此外，对于所有训练数据$x$，$y(x)$和输出$a$近似相等时候，$C(w,b)$会变得很小，例如，$C(w,b) \approx 0$。因此如果我们的训练算法能找到合理的权重和偏移使得$C(w,b) \approx 0$，这将是一个很好的算法。相反，如果$C(w,b)$很大——这意味着$y(x)$和大量输出$a$相差较大。所以训练算法的目标就是找到合适的权重和偏移来最小化$C(w,b)$。换句话说，我们想找到一组权重和偏移集合，使得代价函数值尽可能小。我们将使用梯度下降算法来实现。</p>
<p>为什么引入二次型代价函数？毕竟，我们不是主要关注在网络对多少图像进行了正确分类？为什么不直接最大化这个数值，而不是最小化一个像二次型代价函数一样的间接测量值？问题就在于正确识别的图像数量不是权重和偏移的平滑的函数。对于大多数，权重和偏移的很小改变不会让正确识别的图像数量值有任何改变。这就使得很难指出如何改变权重和偏移来提高性能。如果我们使用一个像二次型的平滑代价函数，它将很容易指出如何细微的改变权重和偏移来改进代价函数。这就是为什么我们首先关注在最小化二次型代价函数，而且只有那样我们才能检测分类的准确性。</p>
<p>即使我们想使用一个平滑的代价函数，你可能仍然想知道为什么在等式$\color{lime}{\eqref{(6)}}$中选择了二次型代价函数。它难道不是一个临时的选择？也许如果我们选择一个不同的代价函数，我们将获取到完全不同的最小化权重和偏移？这是一个很好的考虑，后面我们将回顾这个代价函数，并且进行一些改变。但是，这个等式$\color{lime}{\eqref{(6)}}$中的二次型代价函数对理解神经网络学习基础非常好，因此目前我们将坚持使用它。</p>
<p>简要回顾一下，我们训练神经网络的目标是找出能最小化二次型代价函数$C(w, b)$的权重和偏移。这是一个适定问题，但是它会产生许多当前提出的干扰结构——权重$w$和偏移$b$的解释、背后隐藏的$\sigma$函数、网络架构的选择、MNIST等等。结果说明我们能通过忽略许多这类结构来理解大量内容，且只需要集中在最小化方面。因此现在我们将忘掉所有关于代价函数的特殊形式，神经网络的连接关系等等。相反，我们将想象成只简单的给出一个具有许多变量的函数，且我们想要最小化它的值。我们将开发一个新技术叫梯度下降，它能被用来解决这类最小化问题。然后我们将回到神经网络中这个想最小化的特殊函数。</p>
<p>好的，假设我们将最小化一些函数$C(v)$。它可能是具有许多变量$v = v_1, v_2, \ldots$的任意真实值函数。请注意，我们将用$v$替换符号$w$和$b$来强调它可以适合任意一个函数——我们并不是一定在神经网络的背景下考虑。为了最小化$C(v)$，可以把$C$想象成只具有两个变量，即$v_1$和$v_2$：<br><img src="/imgs2018/9_9_valley.png"><br>我们想要找到怎样才能使$C$达到全局最小化。现在，当然，对上面绘制的函数，我们能看出该图中最小化位置。从这种意义上讲，我可能展示了一个太简单的函数！一个通常的函数$C$可能由许多变量构成，并且很难看出该函数最小化位置在哪里。</p>
<p>一种解决办法就是使用微积分来解析地找到这个最小值。我们能计算导数，然后使用它们来找到$C$函数最小极值的位置。当$C$只有一个或少量变量时，这个办法可能行得通。但是当我们有许多变量时，这将变成一场噩梦。且对于神经网络，我们通常需要更多的变量——最大的神经网络在极端情况下，具有依赖数十亿权重和偏移的代价函数。使用微积分来求最小值显然行不通！</p>
<p>(在断言我们通过把$C$当作只具有两个变量的函数来获得领悟后，我将用两段第二次转回来，并且说：“嘿，一个具有超过两个变量的函数是什么呢？”，对此很抱歉。请相信我，把$C$想象成只具有两个变量的函数对我们理解很有帮助。有时候想象会中断，且最后两段将处理这种中断。好的数学思维通常会涉及应付多个直观想象，学会什么时候合适使用每一个想象，什么时候不合适。)</p>
<p>Ok，所以用微分解析的方法行不通。幸运的是，我们可以通过一种非常直观的类比来找到一种“行得通”的算法。首先将我们的函数看作是一个凹形的山谷。（瞄一眼上面的插图，这种想法应该是很直观的。）好，现在让我们想象有一个小球沿着山谷的坡面向低处滚。生活经验告诉我们这个小球最终会到达谷底。或许我们可以采用类似的思路找到函数的最小值？好，首先我们将随机选取一个点作为这个（想象中的）球的起点，然后再模拟这个小球沿着坡面向谷底运动的轨迹。我们可以简单地通过计算 $C$ 的偏导数（可能包括某些二阶偏导数）来进行轨迹的模拟——这些偏导数蕴涵了山谷坡面局部“形状”的所有信息，因此也决定了小球应该怎样在坡面上滚动。</p>
<p>根据我刚才所写的内容，你可能会以为我们将从小球在坡面上滚动的牛顿运动方程出发，然后考虑重力和坡面阻力的影响，如此等等……实际上，我们不会把“小球在坡面滚动”的这一类比太过当真——毕竟，我们我们的初衷是要设计一种最小化 $C$ 的算法，而不是真地想精确模拟现实世界的物理定律！“小球在坡面滚动”的直观图像是为了促进我们的理解和想象，而不应束缚我们具体的思考。所以，与其陷入繁琐的物理细节，我们不妨这样问自己：如果让我们来当一天上帝，那么我们会怎样设计我们自己的物理定律来引导小球的运动？我们该怎样选择运动定律来确保小球一定会最终滚到谷底？</p>
<p>为了将这个问题描述得更确切，现在让我们来想想，当我们将小球沿着 $v_1$ 方向移动一个小量$\Delta v_1$，并沿着 $v_2$ 方向移动一个小量 $\Delta v_2$ 之后会发生什么。微分法则告诉我们，$C$ 将作如下改变: </p>
<script type="math/tex; mode=display">
    \Delta C\approx 
    \frac{\partial C}{\partial v_1}\Delta v_1+
    \frac{\partial C}{\partial v_2}\Delta v_2
    \tag{7}\label{(7)}</script><p><small>2018-09-09 20:05:12</small><br>我们将设法选择 $\Delta v_1$ 和 $\Delta v_2$ 以确保 $\Delta C$ 是负数，换句话说，我们将通过选择 $\Delta v_1$ 和 $\Delta v_2$ 以确保小球是向着谷底的方向滚动的。为了弄清楚该怎样选择 $\Delta v_1$ 和 $\Delta v_2$，我们定义一个描述 $v$ 改变的矢量 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，这里的 $T$ 同样是 转置（transpose）算符，用来将一个行矢量转化为列矢量。我们还将定义 $C$ 的“梯度”，它是由 $C$ 的偏导数构成的矢量，$\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$ 。我们将“梯度”矢量记为 $\nabla C$，即：</p>
<script type="math/tex; mode=display">
    \nabla C\equiv 
    \left(
    \frac{\partial C}{\partial v_1},
    \frac{\partial C}{\partial v_2}
    \right)^T
    \tag{8}\label{(8)}</script><p>很快我们将会用 $\Delta v$ 和梯度 $\nabla C$ 来重写 $\Delta C$ 的表达式。在这之前，为了避免读者可能对“梯度”产生的困惑，我想再多做一点解释。当人们首次碰到$\nabla C$这个记号的时候，可能会想究竟该怎样看待$\nabla$这个符号。$\nabla$的含义到底是什么？其实，我们完全可以把$\nabla C$整体看作是单一的数学对象——按照上述公式定义的矢量，仅仅是偶然写成了用两个符号标记的形式。根据这个观点，$\nabla$像是一种数学形式的旗语，来提醒我们“嘿，$\nabla C$是一个梯度矢量”，仅此而已。当然也有更抽象的观点，在这种观点下，$\nabla$被看作是一个独立的数学实体（例如，被看作一个微分算符），不过我们这里没有必要采用这种观点。<br>使用上述定义，$\Delta C$ 的表达式 (7) 可以被重写为：</p>
<script type="math/tex; mode=display">
    \Delta C\approx
    \nabla C \cdot \Delta v
    \tag{9}\label{(9)}</script><p>这个方程有助于我们理解为什么 $\nabla C$ 被称为“梯度”矢量：$\nabla C$ 将 $v$ 的改变和 $C$ 的改变联系在了一起，而这正是我们对于“梯度”这个词所期望的含义。然而，真正令我们兴奋的是这个方程让我们找到一种 $\Delta v$ 的选择方法可以确保 $\Delta C$ 是负的。具体来说，如果我们选择 </p>
<script type="math/tex; mode=display">
    \Delta v=-\eta \nabla C
    \tag{10}\label{(10)}</script><p>这里 $\eta$ 是一个正的小参数，被称为“学习率”（learning rate)。那么由方程$\color{lime}{\eqref{(9)}}$可知$\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta ||\nabla C||^2$。如果我们根据公式$\color{lime}{\eqref{(10)}}$指定 $v$ 的改变，由于 $|| \nabla C ||^2 \geq 0$ ，它确保了 $\Delta C \leq 0$，即 $C$ 的值将一直减小，不会反弹。（当然要在近似关系$\color{lime}{\eqref{(9)}}$ ) 成立的范围内）。这正是我们期望的性质！因此，我们将在梯度下降算法（gradient descent algorithm）中用公式 $\color{lime}{\eqref{(10)}}$来定义小球的“运动定律”。也就是说，我们将用公式$\color{lime}{\eqref{(10)}}$来计算 $\Delta v$ 的值，然后将小球的位置 $v$ 移动一小步：</p>
<script type="math/tex; mode=display">
    v \to v'=v - \eta \nabla C
    \tag{11}\label{(11)}</script><p>然后，我们使用这一规则再将小球移动一小步。如果我们不断这样做，$C$ 将不断减小，符合期望的话，$C$ 将一直减小到全局最小值。</p>
<p>总之，梯度下降算法（gradient descent algorithm）是通过不断计算梯度$\nabla C$，然后向着梯度相反的方向小步移动的方式让小球不断顺着坡面滑向谷底。这个过程可以形象地用下图表示：<br><img src="/imgs2018/9_13_valley_with_ball.png"><br>注意：采用这一规则，梯度下降算法并没有模拟真实的物理运动。在现实世界，小球具有动量，而动量可能让小球偏离最陡的下降走向，甚至可以让其暂时向着山顶的方向逆向运动。只有考虑了摩擦力的作用，才能确保小球是最终落在谷底的。但这里，我们选择$\Delta v$的规则却是说“立马给我往下滚”，而这个规则仍然是寻找最小值的一个好方法！</p>
<p>为了让“梯度下降法”能够正确发挥作用，我们必须选择一个足够小的“学习率” $\eta$ 以确保方程$\color{lime}{\eqref{(9)}}$是一个好的近似。如果我们不这样做，最终可能会导致 $\Delta C &gt; 0$，这当然不是我们想要的。与此同时，我们也不希望 $\eta$ 太小，因为η太小的话每一步的 $\Delta v$ 就会非常小, 从而导致梯度下降算法的效率非常低。在实际运用过程中，$\eta$经常是变化的从而确保不仅方程$\color{lime}{\eqref{(9)}}$是好的近似，而且算法也不会太慢。后面我们将会理解具体的做法。</p>
<p>前面我们已经解释了当函数 $C$ 仅仅依赖两个变量时的梯度下降算法。但其实，当 $C$ 依赖更多变量的时候，前面所有的论述依然成立。具体来说，假设函数 $C$ 依赖$m$个变量， $v_1,\ldots,v_m$，那么由微小改变 $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$ 引起的 $C$ 的变化$\Delta C$是: </p>
<script type="math/tex; mode=display">
    \Delta C\approx
    \nabla C\cdot \Delta v
    \tag{12}\label{(12)}</script><p>这里梯度矢量 $\nabla C$ 定义为:</p>
<script type="math/tex; mode=display">
    \nabla C\equiv
    \left(
    \frac{\partial C}{\partial v_1},\ldots ,\frac{\partial C}{\partial v_m}
    \right)^T
    \tag{13}\label{(13)}</script><p>类似于两个自变量的情形，我们可以选择 </p>
<script type="math/tex; mode=display">
    \Delta v=-\eta \nabla C
    \tag{14}\label{(14)}</script><p>那么就可以确保由公式$\color{lime}{\eqref{(12)}}$得到的 $\Delta C$ 是负的。这给了我们一种沿着梯度找到最小值的方法，即使 $C$ 依赖于很多变量。即通过不断重复地使用以下更新规则，</p>
<script type="math/tex; mode=display">
    v \to v'=v - \eta \nabla C
    \tag{15}\label{(15)}</script><p>你可以把这种更新规则看作是“梯度下降算法”的定义。它给了我们一种不断改变 $v$ 的位置从而找到函数 $C$ 最小值的方法。这一规则并不总是有效——有些情况可能会导致出错，阻止梯度下降算法找到 $C$ 的全局最小值。在后面的章节我们会回过头来讨论这种可能性。不过，在实际应用中，梯度下降算法常常非常有效，并且在神经网络中，我们发现它也是一种最小化代价函数（cost function）的强有力方法，因此也有助于神经网络的学习。</p>
<p>的确，从某种意义上说梯度下降其实是寻找最小值的最佳策略。假设我们正试图通过移动一小步 $\Delta v$ 来让函数 $C$ 尽可能地减小。这等价于要要最小化 $\Delta C \approx \nabla C \cdot \Delta v$。我们将限定移动步幅的大小，即令 $|| \Delta v || = \epsilon$ 其中 $\epsilon &gt; 0$ 是一个固定的小量. 换句话说，我们想移动步长限定的一小步，并试图找到一个移动方向能够让C尽可能减小。可以证明这里最小化$\nabla C \cdot \Delta v$的$\Delta v$选择是 $\Delta v = - \eta \nabla C$, 这里 $\eta = \epsilon / ||\nabla C||$ 是由步长约束 $||\Delta v|| = \epsilon$ 所要求的。因此梯度下降可以被认为是一种沿着 $C$ 局域下降最快的方向一步一步向前走，从而找到最小值的方法。</p>
<p><strong>练习</strong><br>1.证明上面最后一段的断言。提示：如果你还不熟悉柯西-施瓦茨不等式，你最好学习一下，这会对你很有帮助。</p>
<p>2.我们阐述当$C$有两个变量时的梯度下降，也阐述了超过两个变量的梯度下降。当$C$只有一个变量时候，会发生什么呢？你能提供在一个维度上梯度下降的几何解释吗？</p>
<p>人们调研过很多从梯度下降变种而来的算法，包括精确模仿小球在真实物理世界运动的方法。这些模仿小球真实物理运动的方法有一些优点，但是也有一个显著的缺点：需要计算 $C$ 的二阶偏导数，而那样的话，计算成本会很高。为了看清楚为什么计算成本会很高，假设我们要计算所有的二阶偏导数 $\partial^2 C/ \partial v_j \partial v_k$。 如果这里需要计算的 $v_j$ 变量有一百万个，我们就要计算大概万亿（百万的平方）量级的二阶偏导数。这个计算成本非常高。 正因为上面所说的，人们搞出不少小技巧来避免计算成本过高的问题，同时，寻找梯度下降的替代算法也是非常活跃的研究领域。不过，本书将始终把梯度下降（及其变种）算法作为我们训练神经网络的主要方法。</p>
<p>我们怎样把梯度下降应用于神经网络的学习过程呢？具体思路是用梯度下降来寻找权重（weights）$w_k$ 和 偏移（bias） $b_l$ ，从而最小化代价函数$\color{lime}{\eqref{(6)}}$。为了看得更清楚，我们将梯度下降更新规则中的变量$v_j$用权重和偏移替换，进行重新表述。换句话说，我们现在的“位置”有 $w_k$ 和 $b_l$ 这些分量, 而梯度矢量 $\nabla C$ 也具有相应的分量 $\partial C / \partial w_k$ 和 $\partial C / \partial b_l$。将梯度下降规则用这些分量的形式写出来，我们有</p>
<script type="math/tex; mode=display">
    w_k \to w_k'=w_k - \eta \frac{\partial C}{\partial w_k}
    \tag{16}\label{(16)}</script><script type="math/tex; mode=display">
    b_l \to b_l'=b_l - \eta \frac{\partial C}{\partial b_l}
    \tag{17}\label{(17)}</script><p>通过不断重复应用这一更新规则，我们可以“从山上滚下来”，符合预期的话，我们将找到代价函数的最小值。换句话说，这个规则可以用于神经网络的学习过程。</p>
<p>在运用梯度下降规则的时候会碰到许多挑战难题。在后面的章节我们会仔细研究这些困难。但这里我只想提及其中的一个难题。为了更好地理解是什么难题，让我们回过头来看一下公式$\color{lime}{\eqref{(6)}}$定义的二次型代价函数。注意到这个代价函数具有形式$C = \frac{1}{n} \sum_x C_x$, 即，每一个训练样例误差 $C_x \equiv \frac{||y(x)-a||^2}{2}$ 的平均。实际上，为了计算梯度 $\nabla C$ ，我们需要对每个训练输入（trainning input）分别计算其梯度 $\nabla C_x$，然后再对它们求平均。不幸的是，如果训练输入的样本数量非常大，那么这样会消耗很多时间，因此导致神经网络的学习过程非常慢。</p>
<p>有一个可用于加速学习过程的办法被称作 “随机梯度下降”（stochastic gradient descent）。 这个办法是通过随机选择训练输入的少量样本，并只计算这些样本的$\nabla C_x$的平均值来估算梯度 $\nabla C$。通过计算这些少量样本的平均值，我们可以很快估算出梯度$\nabla C$的真实值，从而加速梯度下降过程，即加速神经网络的学习过程。</p>
<p>具体来说， 首先“随机梯度下降” 要随机挑出 $m$（一个较小的数目）个训练输入。我们将这些随机的训练输入标记为 $X_1,X_2…X_m$, 并将它们称为 一个 “小组”（mini-batch）。假设样本数$m$足够大，我们预计这个随机小组 $\nabla C_{X_j}$ 的平均值应该和所有 $\nabla C_x$ 的平均值大致相等，即</p>
<script type="math/tex; mode=display">
    \frac{\sum_{j=1}^m\nabla C_{X_j}}{m}  
    \approx \frac{\sum_x\nabla C_x}{n} 
    = \nabla C
    \tag{18}\label{(18)}</script><p>这里第二个求和符号表示对整个训练数据集的求和。交换等号两边的顺序，我们有</p>
<script type="math/tex; mode=display">
    \nabla C \approx \frac{1}{m}\sum_{j=1}^{m}\nabla C_{X_j}
    \tag{19}\label{(19)}</script><p>这个公式明确了我们可以通过仅仅计算随机选择的“小组”梯度来估算整体的梯度值。</p>
<p>为了将其和神经网络的学习过程直接联系起来，假设我们用 $w_k$ 和 $b_l$ 标记神经网络中的权重（weights）和 偏移（biases），那么随机梯度下降的工作方式为：随机挑出一“小组”训练输入，并用如下方式来训练</p>
<script type="math/tex; mode=display">
    w_k \to w_k'=w_k - 
    \frac{\eta}{m} 
    \sum_j\frac{\partial C_{X_j}}{\partial w_k}
    \tag{20}\label{(20)}</script><script type="math/tex; mode=display">
    b_l \to b_l'=b_lk - 
    \frac{\eta}{m} 
    \sum_j\frac{\partial C_{X_j}}{\partial b_l}
    \tag{21}\label{(21)}</script><p>这里的求和是对当前小组里的所有训练样本 $X_j$ 进行的。然后我们再随机挑出另一小组进行训练。如此反复下去，直到我们用尽了所有的训练输入。这个过程被称为完成了一“代”（epoch）训练。到那个时候，我们就可以重新开始另一代训练。</p>
<p>有的时候，我们需要注意：人们在标定代价函数(cost function)和对权重、偏移进行“组”(mini-batch)更新时所采用的惯例可能有所差异。公式$\color{lime}{\eqref{(6)}}$中，我们用了一个 $\frac{1}{n}$ 的因子对整体代价函数进行了标定。有时候，人们会忽略这个 $\frac{1}{n}$ 因子，即对所有训练样例“求和”而不是“求平均”。这一点在训练样例总数不能提前确定的时候非常有用，比如当更多的训练数据在不断实时产生的时候就会发生这种情况。同样类似，公式$\color{lime}{\eqref{(20)}}$和$\color{lime}{\eqref{(21)}}$中的“组”更新规则，有时候也会忽略求和前面的$\frac{1}{m}$因子。从概念上说，两者几乎没什么不同，因为这等价于重新标定了学习率 $\eta$ 而已。不过当我们在仔细对比不同的研究工作时，需要对此留心一点。</p>
<p>我们可以将随机梯度下降看做是某种类似于民意调查的过程：将梯度下降应用于一“小组”样例要比应用于全部样例要容易得多，就如同采样民意调查要比全民公投要容易得多。例如，在MNIST样例中，整个训练集的样本数 $n=60000$ 个，而比如说我们选定的“小组”样本数为$m=10$个。这意味着我们在估算梯度的时候会快$6000$倍！当然，估算结果不一定精确——因为有统计涨落的误差——不过，我们不需要它精确：因为我们关心的仅仅是这个过程是不是整体向着$C$减小的方向在走，而不是要严格计算出梯度值。实际上，随机梯度下降在训练神经网络的过程中是非常常见和有效的手段，也是本书将要演绎的许多训练技巧的基础。<br><strong>练习</strong><br>梯度下降的一种极端方式是使用大小为1的“小组”。也就是，给定一个训练输入$x$，我们按照规则$w_k \rightarrow w_k’ = w_k - \eta \partial C_x / \partial w_k$和$b_l \rightarrow b_l’ = b_l - \eta \partial C_x / \partial b_l$来更新权重和偏差。然后选择另外一个训练输入，再来更新权重和偏差。反复这样，这个过程被称为在线，在线或增量学习。在线学习中，神经网络每次从一个训练输入中进行学习（就像人一样）。请指出在线学习的优点和缺点，相对于具有“小组”大小为$20$的随机梯度下降算法。</p>
<p>好，这一节的最后，我们来讨论一个对于刚刚接触梯度下降算法的人常有的困扰。在神经网络中，代价函数 $C$ 当然依赖于很多变量，即，所有的权重和偏移。因此，从某种意义上说，$C$ 定义了一个非常非常高维空间中的曲面。有些人可能会觉得；“嗯，我需要想象这些额外的维度”，然后开始担心：“我甚至连四维空间都没法想象，更别说五维或者五百万维了”。是不是这些人缺少了某种“真正的”超级数学家所具有的超能力？当然不是。即便是最牛哄哄的数学家也很难想象四维空间，如果可以的话。技巧在于这些数学家发展了一套描述问题的替代方法，而这正是我们刚才所做的：我们用一套代数形式的（而非直观的）算法来表征 $\Delta C$，从而进一步思考怎样让$C$不断减小。那些擅长思考高维空间的人，其实头脑中内建了一个“库”，这个“库”包含了许多类似于刚才描述的这些抽象方法。这些方法也许不像我们所熟知的三维空间那样简单直观，不过一旦你在头脑中也建立了这个“库”，你就可以驾轻就熟地“想象”高维空间了。这里我就不再深入讨论下去了，如果你对于数学家们怎样想象高维空间感兴趣的话，你可能会喜欢读<a href="https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking" target="_blank" rel="noopener">这篇讨论</a>。虽然其中讨论到的一些方法非常复杂，但主要的方法还是比较直观，也可以被大家掌握的。</p>
<h3 id="1-8使用神经网络识别手写数字"><a href="#1-8使用神经网络识别手写数字" class="headerlink" title="1.8使用神经网络识别手写数字"></a>1.8使用神经网络识别手写数字</h3><p>好了，让我们使用随机梯度下降算法和MNIST训练数据来写一个能够学习识别手写数字的程序。首先我们需要获得MNIST数据。如果你是git的使用者，你可以通过克隆这本书的源代码库来获得这些数据,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</span><br></pre></td></tr></table></figure></p>
<p>如果你不使用git，你可以通过<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip" target="_blank" rel="noopener">这里</a>下载所需的数据和源代码。<br>顺便说一下，早些时候我在描述MNIST数据时，我说过它被分成了两份，其中60000幅图片用于训练，另外10000幅用于测试。那是MNIST官方的陈述。事实上，我们将要使用的分法会有一点区别。用于测试的数据我们不会改动，但是60000幅用于训练的图片将被分成两部分：其中一份包括50000幅图片，将用于训练我们的神经网络，剩下的10000幅图片将作为验证集（validation set）。虽然我们在这一章不会用这个验证集，但是在这本书之后的部分我们会发现验证集对确定怎么设置神经网络的一些超参数(hyper－parameters)很有用，比如学习速率等，这些参数没有在我们的学习算法中直接选取。尽管验证集不是MNIST数据的原始规范，但是很多人都是用这种方式使用MNIST，验证集的使用在神经网络中非常普遍。从现在开始，当我说到“MNIST训练数据集”时，我指的是我们分出来的50000幅图片，而不是原来的60000幅图片集。<br>除去MNIST数据，我们还需要一个叫Numpy的Python函数库用于做快速线性代数运算。如果你还没有安装Numpy，你通过<code>pip install numpy</code>安装它。</p>
<p>在给出完整的列表之前，下面让我先解释一下神经网络程序的核心特征。最重要的部分就是一个叫Network的类（class），它被用来表示一个神经网络。下面是用于初始化Network对象的代码:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x) </span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure></p>
<p>在这段代码中，列表sizes包含在对应层的神经元的个数。因此，比如说，如果我们想要构造一个Network对象，其中第一层有2个神经元，第二层有3个神经元，最后一层只有1个神经元，我们可以通过下面的代码做到些：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = Network([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></p>
<p>Network对象中所有的偏移（biases）和权重（weights）都通过Numpy中的np.random.randn函数产生标准正态分布的随机数进行初始化。这个随机的初始化给了随机梯度下降算法一个起始点。在随后的章节中我们会找到更好的初始化的方法，但是现在会用这个方法。注意到Network初始化的代码假设了第一层神经元为输入层，并且没有对那些神经元设置偏移量，因为只有在计算之后层的输出时才用到偏移量。</p>
<p>同时也要注意到，偏移量和权重系数被储存为Numpy矩阵的列表。因此，比如说net.weights[1] 就是一个Numpy矩阵，它储存着连接第2层和第3层神经元的权重系数。（注意不是第1层和第2层，因为python列表的索引指标是从0开始的。）因为net.weights[1]比较冗长，让我们用w表示这个矩阵。矩阵元$w_{jk}$就是连接第2层中第$k$个神经元和第3层中第$j$个神经元的权重系数。索引指标$j$和$k$这样的排序是不是看上去有点奇怪，交换$j$和$k$的顺序是不是看上去更合理？这样排序的最大的好处就是第三层的神经元的激活向量（vector of activations）可以直接写成：</p>
<script type="math/tex; mode=display">
    a'=\sigma (wa+b)
    \tag{22}\label{(22)}</script><p>这个公式描述的过程有点多，因此让我们一点一点地来解开。$a$是第2层神经元的激活向量（vector of activations）。为了得到$a’$，我们先将$a$乘以权重矩阵$w$，然后加上偏移向量$b$，最后我们对向量$wa +b$中的每一个元使用函数$\sigma$。（这叫函数$\sigma$的向量化（vectorizing）。）很容验证，在计算sigmoid神经元的输出结果时，公式$\color{lime}{\eqref{(22)}}$给出的结果会和我们之前用公式$\color{lime}{\eqref{(4)}}$所说的方式给出的结果是一样的。<br><strong>练习</strong><br>将公式$\color{lime}{\eqref{(22)}}$写成向量元素的形式，验证在算sigmoid神经元的输出时，它给出的结果和公式$\color{lime}{\eqref{(4)}}$的结果是一样的。</p>
<p>有了这些在心中，很容易写一段程序用于计算一个Network 对象的输出结果。我们从定义sigmoid函数开始：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br></pre></td></tr></table></figure></p>
<p>注意到，当输入量z是一个向量或者Numpy数组，Numpy自动地对向量的每一个元运用sigmoid函数。</p>
<p>然后我们对Network 类加一个feedforward方法。当一个输入量a 给了network，这个方法就给出对应的输出。这个公式所做的就是将公式$\color{lime}{\eqref{(22)}}$运用到每一层<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">    <span class="string">"""Return the output of the network if "a" is input."""</span></span><br><span class="line">    <span class="string">"""第一层输入，第n层输出"""</span></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">        a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure></p>
<p>当然，我们想让Network对象做的最主要的事情是学习。为了达到这个目的，我们要给它们一个SGD方法，用这个方法实现随机梯度下降算法。下面是代码。有几处地方有一点难以理解，但是我们会在给出代码后加以讲解。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">        test_data=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">    gradient descent.  The "training_data" is a list of tuples</span></span><br><span class="line"><span class="string">    "(x, y)" representing the training inputs and the desired</span></span><br><span class="line"><span class="string">    outputs.  http://tensorfly.cn/home The other non-optional parameters are</span></span><br><span class="line"><span class="string">    self-explanatory.  If "test_data" is provided then the</span></span><br><span class="line"><span class="string">    network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">    epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">    tracking progress, but slows things down substantially."""</span></span><br><span class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">    n = len(training_data)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">        random.shuffle(training_data)</span><br><span class="line">        mini_batches = [</span><br><span class="line">            training_data[k:k+mini_batch_size]</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">            self.update_mini_batch(mini_batch, eta)</span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                j, self.evaluate(test_data), n_test)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br></pre></td></tr></table></figure></p>
<p>training_data是一个由(x, y)类型元组组成的列表，其中x表示训练数据的输入，y为对应的输出。变量epochs和mini_batch_size正如你所预期的一样,分别是训练的代数和取样时最小组的大小。eta是学习的速率，$\eta$。如果可选参量test_data被提供了，程序在每一个训练“代”（epoch）结束之后都会评估神经网络的表现，然后输出部分进展。这对跟踪进展有用，但是会大大减慢速度。<br><small>2018-09-13 21:42:47</small><br>这段代码按照下面的方式运行。在每一代（epoch），程序首先随机地打乱训练数据的顺序，然后将数据分成合适大小的mini_batch。这个很容，就是随机从训练数据中抽样。接下来，对每一个epochs，我们使用一次梯度下降算法。这个通过self.update_mini_batch(mini_batch, eta)所对应的程序代码实现，这段代码会利用mini_batch中的训练数据，通过一个梯度下降的循环来更新神经网络的权重系数和偏移量。下面是update_mini_batch方法的源代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">    <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">    gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">    The "mini_batch" is a list of tuples "(x, y)", and "eta"</span></span><br><span class="line"><span class="string">    is the learning rate."""</span></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">    self.weights = [w-(eta/len(mini_batch))*nw </span><br><span class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">    self.biases = [b-(eta/len(mini_batch))*nb </span><br><span class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure></p>
<p>大部分工作都是下面这一行做的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br></pre></td></tr></table></figure></p>
<p>这个牵涉到一个叫逆传播（backpropagation）的算法，这个算法可以快速计算代价函数（cost function）的梯度。因此update_mini_batch 所作的仅仅就是对 mini_batch中每一个训练数据样本计算这些梯度，然后更新self.weights和self.biases。</p>
<p>现在我不会展示出self.backprop的源代码。我们会在下一章学习逆传播是怎么工作的，以及self.backprop的源代码。现在，就假设它如上面所说的方式工作，返回每一个训练数据样本x所对应成本的正确梯度。 让我们看一下整个程序，包括之前忽略了的说明文档。除了self.backprop，整个程序是无需解释的－－所有重要的事情是在self.SGD和self.update_mini_batch中做的，我们之前已经讨论过了。self.backprop方法在计算梯度的时候还要用到一些额外的函数，包括sigmoid_prime，用来计算$\sigma$函数的导数，和self.cost_derivative。后面这个我暂时不会在这里描述。只要看一看这些方法的代码或者说明文档，你就可以了解它们的主要意思（甚至细节）。我们将在下一章详细研究它们。注意到这个程序看上去很长，但是大部分都是说明文档，让这个程序更容易理解。实际上，整个程序只有74行非空白、非注释代码。所有的代码可以在<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py" target="_blank" rel="noopener">GitHub</a>上找到。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">network.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A module to implement the stochastic gradient descent learning</span></span><br><span class="line"><span class="string">algorithm for a feedforward neural network.  Gradients are calculated</span></span><br><span class="line"><span class="string">using backpropagation.  Note that I have focused on making the code</span></span><br><span class="line"><span class="string">simple, easily readable, and easily modifiable.  It is not optimized,</span></span><br><span class="line"><span class="string">and omits many desirable features.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won't set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers."""</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially."""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure></p>
<p>这个程序识别手写数字的效果怎么样呢? 好，让我们从加载MNIST数据开始。我将用下面这个小脚本mnist_loader.py来做这件事, 我们将在python shell中运行下面的指令，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> mnist_loader</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = \</span><br><span class="line"><span class="meta">... </span>mnist_loader.load_data_wrapper()</span><br></pre></td></tr></table></figure></p>
<p>当然，这也可以通过一个独立的python程序来完成，不过如果你一直在跟着这个教程走，在python shell中完成应该是最简单的。<br>加载完MNIST数据后，我们将要建立一个有$30$个隐藏神经元的Network。这件事是在我们导入上面的Python程序之后去做的，对应的变量我们称其为network,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> network</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p>
<p>最终，我们将用随机梯度下降法，由MNIST training_data完成学习过程。该过程将历经30代（epoch），其每“组”（mini-batch）的样本数为10，学习率（learning rate）为 $\eta = 3.0$,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure></p>
<p>注意：如果你在一边在读教程一边在跑程序，你会发现这些程序的执行需要花点时间 —— 对于一般的主机（2015年左右），大概需要几分钟。我建议你一边运行，一边继续阅读，并且周期性地查看代码的输出结果。如果你时间比较赶，你可以通过减少学习的epoch数目，减少隐藏神经元的数目，或者只用部分训练数据等方法来加速程序的运行过程。注意用于生产环境的代码（production code）会远远快于现在的速度：这里的python脚本是用来帮助你理解神经网络是怎样工作的，而并非以高效运行为目标的代码！当然，一旦我们将神经网络训练好，我们就可以在几乎任何计算平台上运行得飞快。例如，一旦我们通过神经网络的学习过程获得了一组很好的权重和偏移，我们就可以将其非常容易地移植为网页浏览器的javascript版本，或者是移动设备的原生应用版本。不管怎样，下面是训练神经网络过程的一组结果。这组结果显示了经历每一代训练后，可以正确辨认出来的测试图片的个数。正如你所看到的，经历过第一代训练后，其准确率已经达到10,000张图片中识别出9,129张图片，并且这个数字在不断增加，</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>原文:<a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap1.html</a></p>

      
    </div>
    
    
    

    
      <div>
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="Frey wechat" style="width: 200px; max-width: 100%;"/>
    <div>欢迎您扫一扫上面的微信公众号，订阅我的博客！</div>
</div>

      </div>
    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>谢谢支持</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Frey 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Frey 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/12/DEBUG常用功能/" rel="next" title="DEBUG常用功能">
                <i class="fa fa-chevron-left"></i> DEBUG常用功能
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/28/python3之模块/" rel="prev" title="python3之模块">
                python3之模块 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Frey" />
            
              <p class="site-author-name" itemprop="name">Frey</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/summerIwinter" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:summerIwiner@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://weibo.com/u/1028574570" target="_blank" title="微博">
                    
                      <i class="fa fa-fw fa-globe"></i>微博</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#使用神经网络识别手写数字"><span class="nav-text">使用神经网络识别手写数字</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-第一章"><span class="nav-text">1.第一章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1感知器（Perceptrons）"><span class="nav-text">1.1感知器（Perceptrons）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2Sigmoid神经元（Sigmoid-neurons）"><span class="nav-text">1.2Sigmoid神经元（Sigmoid neurons）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3练习"><span class="nav-text">1.3练习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4神经网络的体系结构"><span class="nav-text">1.4神经网络的体系结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5一个简单的网络来分类手写数字"><span class="nav-text">1.5一个简单的网络来分类手写数字</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6练习"><span class="nav-text">1.6练习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7梯度下降学习算法"><span class="nav-text">1.7梯度下降学习算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8使用神经网络识别手写数字"><span class="nav-text">1.8使用神经网络识别手写数字</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="contact">
  <span class="fa-stack fa-lg">
    <a href="https://www.github.com/summerIwinter" target="view_window">
      <i class="fa fa-square fa-stack-2x"></i> 
      <i class="fa fa-github fa-stack-1x fa-inverse"></i> </a>
  </span>
  <span class="fa-stack fa-lg">
    <a href="https://www.weibo.com/u/1028574570" target="view_window">
    <i class="fa fa-square fa-stack-2x"></i> 
    <i class="fa fa-weibo fa-stack-1x fa-inverse"></i> </a>
  </span>
  <span class="fa-stack fa-lg">
    <a href="https://jianshu.com/u/a94426fb7853" target="view_window">
    <i class="fa fa-square fa-stack-2x"></i> 
    <i class="fa fa-pencil fa-stack-1x fa-inverse"></i> </a>
  </span>
  <span class="fa-stack fa-lg">
    <a href="/images/wechatadd.png" target="view_window">
      <i class="fa fa-square fa-stack-2x"></i> 
      <i class="fa fa-wechat fa-stack-1x fa-inverse"></i> </a>
  </span><span class="fa-stack fa-lg">
    <a href="mailto:summerIwiner@gmail.com" target="view_window">
      <i class="fa fa-square fa-stack-2x"></i> 
      <i class="fa fa-envelope fa-stack-1x fa-inverse"></i></a>
  </span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="本站字数统计">37.7k</span>
  

  
    <span class="site-uv" id="busuanzi_container_site_uv" style='display:none' title="访客数">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv" id="busuanzi_container_site_pv" style='display:none' title="访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  

</div>








        
  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info"><p style="margin: 0 0 0 0">Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p></div>


<div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Frey</span>

  <span class="with-love"><i class="fa fa-cogs"></i> <span>&nbsp;All Rights Reserved.</span></span>
</div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("IYkIGtHfOiXLr2xvQwvikmRk-gzGzoHsz", "Hw0Ta8iM7YvN1RscFx37Dwh1");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


  

  



  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  

  
  <!-- 不蒜子计数初始值纠正 -->
    <script >
        $(document).ready(function() {
          var intpv = setInterval(fixCountpv, 50);  // 50ms周期检测函数
          var intuv = setInterval(fixCountuv, 50);
          var countOffsetpv = 5037;  // 初始化首次数据
          var countOffsetuv = 2275;
          function fixCountpv() {                   
           if ($("#busuanzi_container_site_pv").css("display") != "none"){
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffsetpv); // 加上初始数据
                clearInterval(intpv); // 停止检测
            }
          }
          function fixCountuv() {                   
           if ($("#busuanzi_container_site_uv").css("display") != "none"){
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffsetuv); // 加上初始数据
                clearInterval(intuv); // 停止检测
            }
          }
        });
    </script>
  
  
</body>
</html>
